{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core \n",
    "\n",
    "> Minimal pipeline for Diffusion Guidance experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from abc import ABC\n",
    "import importlib\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm    import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import diffusers\n",
    "from diffusers    import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers    import LMSDiscreteScheduler, DDIMScheduler, EulerDiscreteScheduler, DPMSolverMultistepScheduler, EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "from torch import nn\n",
    "try:\n",
    "    from k_diffusion.external import CompVisDenoiser, CompVisVDenoiser\n",
    "    from k_diffusion.sampling import get_sigmas_karras\n",
    "    import k_diffusion.sampling as k_sampling\n",
    "except:\n",
    "    print(f'WARNING: Could not import k_diffusion')\n",
    "from dynamic_cfg.kdiff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "'''Map from string name to `diffusers` class.'''\n",
    "diff_name2kls = {\n",
    "    'pndm' : diffusers.schedulers.scheduling_pndm.PNDMScheduler, \n",
    "    'ddpm' : diffusers.schedulers.scheduling_ddpm.DDPMScheduler, \n",
    "    'ddim' : diffusers.schedulers.scheduling_ddim.DDIMScheduler, \n",
    "    'euler_ancestral' : diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler, \n",
    "    'heun' : diffusers.schedulers.scheduling_heun_discrete.HeunDiscreteScheduler, \n",
    "    'euler' : diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler, \n",
    "    'lms' : diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler, \n",
    "    'dpm_multi' : diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler, \n",
    "    'dpm_single' : diffusers.schedulers.scheduling_dpmsolver_singlestep.DPMSolverSinglestepScheduler, \n",
    "    'kdpm2': diffusers.schedulers.scheduling_k_dpm_2_discrete.KDPM2DiscreteScheduler,\n",
    "    'kdpm2_ancestral': diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete.KDPM2AncestralDiscreteScheduler,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MinimalDiffusion:\n",
    "    \"\"\"Loads a Stable Diffusion pipeline.\n",
    "    \n",
    "    The goal is to have more control of the image generation loop. \n",
    "    This class loads the following individual pieces:\n",
    "        - Tokenizer\n",
    "        - Text encoder\n",
    "        - VAE\n",
    "        - U-Net\n",
    "        - Sampler\n",
    "        \n",
    "    The `self.generate` function uses these pieces to run a Diffusion image generation loop.\n",
    "    \n",
    "    This class can be subclasses and any of its methods overriden to gain even more control over the Diffusion pipeline. \n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, device, dtype, revision,\n",
    "                 better_vae='', unet_attn_slice=True, schedule_kwargs={}):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.revision = revision\n",
    "        self.generator = None \n",
    "        self.better_vae = better_vae\n",
    "        self.unet_attn_slice = unet_attn_slice\n",
    "        # group the sampler kwargs\n",
    "        self.schedule_kwargs = schedule_kwargs\n",
    "        \n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"Loads and returns the individual pieces in a Diffusion pipeline.        \n",
    "        \"\"\"\n",
    "        # load the pieces\n",
    "        self.load_text_pieces()\n",
    "        self.load_vae()\n",
    "        self.load_unet()\n",
    "        self.load_scheduler()\n",
    "        # put them on the device\n",
    "        self.to_device()\n",
    "    \n",
    "    def load_text_pieces(self):\n",
    "        \"\"\"Creates the tokenizer and text encoder.\n",
    "        \"\"\"\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            subfolder=\"tokenizer\",\n",
    "            torch_dtype=self.dtype)\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            subfolder=\"text_encoder\",\n",
    "            torch_dtype=self.dtype)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder\n",
    "    \n",
    "    \n",
    "    def load_vae(self):\n",
    "        \"\"\"Loads the Variational Auto-Encoder.\n",
    "        \n",
    "        Optionally loads an improved `better_vae` from the stability.ai team.\n",
    "            It can be either the `ema` or `mse` VAE.\n",
    "        \"\"\"\n",
    "        # optionally use a VAE from stability that was trained for longer \n",
    "        if self.better_vae:\n",
    "            assert self.better_vae in ('ema', 'mse')\n",
    "            print(f'Using the improved VAE \"{self.better_vae}\" from stabiliy.ai')\n",
    "            vae = AutoencoderKL.from_pretrained(\n",
    "                f\"stabilityai/sd-vae-ft-{self.better_vae}\",\n",
    "                revision=self.revision,\n",
    "                torch_dtype=self.dtype)\n",
    "        else:\n",
    "            vae = AutoencoderKL.from_pretrained(self.model_name, subfolder='vae',\n",
    "                                                torch_dtype=self.dtype)\n",
    "        self.vae = vae\n",
    "\n",
    "        \n",
    "    def load_unet(self):\n",
    "        \"\"\"Loads the U-Net.\n",
    "        \n",
    "        Optionally uses attention slicing to fit on smaller GPU cards.\n",
    "        \"\"\"\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            subfolder=\"unet\",\n",
    "            #revision=self.revision,\n",
    "            torch_dtype=self.dtype)\n",
    "        # optionally enable unet attention slicing\n",
    "        if self.unet_attn_slice:\n",
    "            print('Enabling default unet attention slicing.')\n",
    "            if isinstance(unet.config.attention_head_dim, int):\n",
    "                # half the attention head size is usually a good trade-off between\n",
    "                # speed and memory\n",
    "                slice_size = unet.config.attention_head_dim // 2\n",
    "            else:\n",
    "                # if `attention_head_dim` is a list, take the smallest head size\n",
    "                slice_size = min(unet.config.attention_head_dim)\n",
    "            unet.set_attention_slice(slice_size)\n",
    "        self.unet = unet\n",
    "        \n",
    "                \n",
    "    def load_scheduler(self):\n",
    "        \"\"\"Loads the scheduler.\n",
    "        \"\"\"\n",
    "        # parse out the name of the scheduler\n",
    "        scheduler_name = self.schedule_kwargs.get('scheduler_kls')\n",
    "        self.use_k_diffusion = scheduler_name.startswith('k_')\n",
    "        \n",
    "        # load the k-diffusion class\n",
    "        if self.use_k_diffusion:\n",
    "            SamplerCls = SAMPLER_LOOKUP[scheduler_name]\n",
    "            self.sampler = SamplerCls(self.unet, self.model_name)\n",
    "            print(f'Using k-diffusion sampler: {self.sampler}')\n",
    "        \n",
    "        # use the huggingface diffusers class\n",
    "        else:\n",
    "            sched_kls = diff_name2kls[scheduler_name]\n",
    "            self.scheduler = sched_kls.from_pretrained(self.model_name, subfolder=\"scheduler\")\n",
    "            print(f'Using diffusers scheduler: {self.scheduler}')\n",
    "\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt,\n",
    "        dynamic_cfg=None,\n",
    "        width=512,\n",
    "        height=512,\n",
    "        num_steps=50,\n",
    "        use_karras_sigmas=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Main image generation loop.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare the text embeddings\n",
    "        text = self.encode_text(prompt)\n",
    "        neg_prompt = kwargs.get('negative_prompt', '')\n",
    "        if neg_prompt:\n",
    "            print(f'Using negative prompt: {neg_prompt}')\n",
    "        else:\n",
    "            print(f'No negative prompt, using empty unconditional input')\n",
    "        uncond = self.encode_text(neg_prompt)\n",
    "        \n",
    "        # start from shared, initial latents\n",
    "        if getattr(self, 'init_latents', None) is None:\n",
    "            self.init_latents = self.get_initial_latents(height, width)\n",
    "        latents = self.init_latents.clone().to(self.unet.device)\n",
    "\n",
    "        # store reference to dynamic guider\n",
    "        self.dynamic_cfg = dynamic_cfg\n",
    "        \n",
    "        # use the k-diffusion library\n",
    "        if self.use_k_diffusion:\n",
    "            latents = self.k_sampling_loop(num_steps, text, uncond, latents, dynamic_cfg)\n",
    "        \n",
    "        # use the diffusers scheduler loop\n",
    "        else:\n",
    "            # set the number of scheduler steps\n",
    "            self.scheduler.set_timesteps(num_steps, device=self.unet.device)\n",
    "            # for dynamic guidance, use the number of total scheduler steps\n",
    "            self.dynamic_cfg.set_timesteps(len(self.scheduler.timesteps))\n",
    "            # prepare the conditional and unconditional inputs\n",
    "            text_emb = torch.cat([uncond, text]).type(self.unet.dtype)\n",
    "            # scale the initial input latents\n",
    "            latents = latents * self.scheduler.init_noise_sigma\n",
    "            # run the diffusion process\n",
    "            for i,ts in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "                latents = self.diffuse_step(latents, text_emb, ts, i)\n",
    "\n",
    "        # decode the final latents and return the generated image\n",
    "        image = self.image_from_latents(latents)\n",
    "        return image    \n",
    "\n",
    "\n",
    "    def diffuse_step(self, latents, text_emb, ts, idx):\n",
    "        \"\"\"Runs a single diffusion step.\n",
    "        \"\"\"\n",
    "        inp = self.scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "        with torch.no_grad(): \n",
    "            tf = ts\n",
    "            if torch.has_mps:\n",
    "                tf = ts.type(torch.float32)\n",
    "            preds = self.unet(inp, tf, encoder_hidden_states=text_emb)\n",
    "            u, t  = preds.sample.chunk(2)\n",
    "        \n",
    "        # run classifier-free guidance\n",
    "        pred = self.dynamic_cfg.guide(u, t, idx)\n",
    "        \n",
    "        # update and return the latents\n",
    "        latents = self.scheduler.step(pred, ts, latents).prev_sample\n",
    "        return latents\n",
    "    \n",
    "    \n",
    "    def k_sampling_loop(self, num_steps, text, uncond, latents):\n",
    "        '''Run the k-diffusion sampling loop.\n",
    "        '''\n",
    "        # move wrapped sigmas and log-sigmas to device\n",
    "        self.sampler.cv_denoiser.sigmas = self.sampler.cv_denoiser.sigmas.to(latents.device)\n",
    "        self.sampler.cv_denoiser.log_sigmas = self.sampler.cv_denoiser.log_sigmas.to(latents.device)\n",
    "\n",
    "        # sample with k_diffusion\n",
    "        latents = self.sampler.sample(\n",
    "            num_steps=num_steps,\n",
    "            initial_latent=latents,\n",
    "            positive_conditioning=text,\n",
    "            neutral_conditioning=uncond,\n",
    "            t_start=None,#t_enc,\n",
    "            mask=None,#mask,\n",
    "            orig_latent=None,#init_latent,\n",
    "            shape=latents.shape,\n",
    "            batch_size=1,\n",
    "            dynamic_cfg=self.dynamic_cfg,\n",
    "            use_karras_sigmas=self.schedule_kwargs.get('use_karras_sigmas', False),\n",
    "        )\n",
    "        return latents\n",
    "    \n",
    "\n",
    "    def encode_text(self, prompts, maxlen=None):\n",
    "        \"\"\"Extracts text embeddings from the given `prompts`.\n",
    "        \"\"\"\n",
    "        maxlen = maxlen or self.tokenizer.model_max_length\n",
    "        inp = self.tokenizer(prompts, padding=\"max_length\", max_length=maxlen, \n",
    "                             truncation=True, return_tensors=\"pt\")\n",
    "        inp_ids = inp.input_ids.to(self.device)\n",
    "        return self.text_encoder(inp_ids)[0]\n",
    "\n",
    "    \n",
    "    def to_device(self, device=None):\n",
    "        \"\"\"Places to pipeline pieces on the given device\n",
    "        \n",
    "        Note: assumes we keep Scheduler and Tokenizer on the cpu.\n",
    "        \"\"\"\n",
    "        device = device or self.device\n",
    "        for m in (self.text_encoder, self.vae, self.unet):\n",
    "            m.to(device)\n",
    "    \n",
    "    \n",
    "    def set_initial_latents(self, latents):\n",
    "        \"\"\"Sets the given `latents` as the initial noise latents.\n",
    "        \"\"\"\n",
    "        self.init_latents = latents\n",
    "        \n",
    "        \n",
    "    def get_initial_latents(self, height, width):\n",
    "        \"\"\"Returns an initial set of latents.\n",
    "        \"\"\"\n",
    "        return torch.randn((1, self.unet.in_channels, height//8, width//8),\n",
    "                           dtype=self.dtype, generator=self.generator)\n",
    "    \n",
    "    \n",
    "    def image_from_latents(self, latents):\n",
    "        \"\"\"Scales diffusion `latents` and turns them into a PIL Image.\n",
    "        \"\"\"\n",
    "        # scale and decode the latents\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        with torch.no_grad():\n",
    "            data = self.vae.decode(latents.type(self.vae.dtype)).sample[0]\n",
    "        # Create PIL image\n",
    "        data = (data / 2 + 0.5).clamp(0, 1)\n",
    "        data = data.cpu().permute(1, 2, 0).float().numpy()\n",
    "        data = (data * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(data)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('sdiff2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6f7779b98fa77174ab8272fbd2d8ed63d72f12d6b0a519fd3f1b9efd99bf429"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
